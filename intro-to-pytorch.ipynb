{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to PyTorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure style='float:right;max-width:30%;'>\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/c/c6/PyTorch_logo_black.svg/640px-PyTorch_logo_black.svg.png' style='padding:10px;background-color:white'>\n",
    "<figcaption style='text-align:right'>Source: <a href=https://commons.wikimedia.org/wiki/File:PyTorch_logo_black.svg>Wikimedia Commons</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "PyTorch is a machine learning framework with a major focus on neural networks used for computer vision, audio and natural language processing. The user-facing frontend is written in Python, but the number-crunching is handled by a more optimized C++ backend, including support for outsourcing computations to graphics cards (GPUs) for a substantial increase in speed. PyTorch was originally created by Meta (formerly known as facebook), but has always been open source, permissively licensed ([BSD-3](https://en.wikipedia.org/wiki/BSD_licenses#3-clause)), and since September 2022 is managed by the non-profit PyTorch Foundation, a subsidiary of the [Linux Foundation](https://en.wikipedia.org/wiki/Linux_Foundation).\n",
    "\n",
    "The accessible interface, huge community, and optimized implementations have established PyTorch among the top choices for education, research, and production in the field of neural network design.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "\n",
    "<b>Caveat emptor:</b> PyTorch is not really \"better\" or \"worse\" than other popular frameworks like [Keras](https://keras.io/) or [TensorFlow](https://www.tensorflow.org/). While each framework has its particular strengths, they differ more in their style, philosophy, and user base than in their feature lists and performance. You should absolutely explore other options available to you and find what you like best!\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prerequisites"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is directed at an audience with a fundamental understanding of neural networks. If you are completely new to this field, you might want to check out the following articles:\n",
    "\n",
    "- [A Gentle Introduction to Neural Networks](https://towardsdatascience.com/a-gentle-introduction-to-neural-networks-series-part-1-2b90b87795bc)\n",
    "- [Build Neural Network from Scratch](https://towardsdatascience.com/build-neural-network-from-scratch-part-2-673ec7cdd89f)\n",
    "- [Machine Learning for Beginners: An Introduction to Neural Networks](https://towardsdatascience.com/machine-learning-for-beginners-an-introduction-to-neural-networks-d49f22d238f9)\n",
    "\n",
    "A good command of the Python programming language is also required to fully understand the code presented in this notebook. If you have no experience with Python, there are innumerable great tutorials and introductions out there. You can find a good overview of some of them [here](https://wiki.python.org/moin/BeginnersGuide/Programmers)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Structure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core package of PyTorch is called [`torch`](https://pypi.org/project/torch/). This package contains all the code required to setup and compute general purpose neural networks. It is extended by packages that offer more specialized functions and objects specific to various applications: [`torchvision`](https://pytorch.org/vision/stable/index.html) for Computer Vision (working with images or videos), [`torchaudio`](https://pytorch.org/audio/stable/index.html) for audio processing (e.g. speech recognition or synthesis), and [`torchtext`](https://pytorch.org/text/stable/index.html) for natural language processing. PyTorch is extended by various other packages that comprise the [*PyTorch Ecosystem*](https://pytorch.org/ecosystem/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Core components"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plethora of functions and objects can be found within PyTorch. But arguably the most important basic components are:\n",
    "\n",
    "1. The [`Tensor`](https://pytorch.org/docs/stable/tensors.html#tensor-class-reference) class\n",
    "2. The differentiation engine [`Autograd`](https://pytorch.org/docs/stable/autograd.html#module-torch.autograd)\n",
    "3. The neural network building blocks (layers and activation functions) found in [`torch.nn`](https://pytorch.org/docs/stable/nn.html#module-torch.nn)\n",
    "\n",
    "Before we build our first neural network from scratch, let us walk through these components one at a time:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 The `Tensor` class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure style='float:right;max-width=10%;'>\n",
    "<img src=https://imgs.xkcd.com/comics/machine_learning.png style='padding-right:10px'>\n",
    "<figcaption>Source: <a href=https://xkcd.com/license.html>XKCD</a> </figcaption>\n",
    "</figure>\n",
    "\n",
    "Neural networks are essentially a sequence of linear algebra operations. A [mathematical tensor](https://en.wikipedia.org/wiki/Tensor) is the most general algebraic object, of which simpler algebraic objects can be derived:\n",
    "\n",
    "- A scalar is a tensor of rank 0:\n",
    "$$\n",
    "\\left[ 0 \\right]\n",
    "$$\n",
    "- A vector is a tensor of rank 1 (a.k.a. a collection of rank 0 tensors):\n",
    "$$\n",
    "\\begin{bmatrix} \\left[ 0 \\right], \\left[ 1 \\right], \\left[ 2 \\right] \\end{bmatrix}\n",
    "$$\n",
    "- A matrix is a tensor of rank 2 (a.k.a. a collection of rank 1 tensors):\n",
    "$$\n",
    "\\begin{bmatrix} \n",
    "  \\begin{bmatrix} \\left[ 0 \\right], \\left[ 1 \\right], \\left[ 2 \\right] \\end{bmatrix} \\\\\n",
    "  \\begin{bmatrix} \\left[ 3 \\right], \\left[ 4 \\right], \\left[ 5 \\right] \\end{bmatrix} \\\\\n",
    "  \\begin{bmatrix} \\left[ 6 \\right], \\left[ 7 \\right], \\left[ 8 \\right] \\end{bmatrix} \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "- An $n$-dimensional array is a tensor of rank $n$ (a.k.a. a collection of rank $n-1$ tensors):\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "  \\begin{bmatrix} \\left[ 0 \\right], \\left[ 1 \\right], \\left[ 2 \\right] \\end{bmatrix} \\\\\n",
    "  \\begin{bmatrix} \\left[ 3 \\right], \\left[ 4 \\right], \\left[ 5 \\right] \\end{bmatrix} \\\\\n",
    "  \\begin{bmatrix} \\left[ 6 \\right], \\left[ 7 \\right], \\left[ 8 \\right] \\end{bmatrix} \n",
    "\\end{bmatrix}, \n",
    "\\begin{bmatrix} \n",
    "  \\begin{bmatrix} \\left[ 9 \\right], \\left[ 10 \\right], \\left[ 11 \\right] \\end{bmatrix} \\\\\n",
    "  \\begin{bmatrix} \\left[ 12 \\right], \\left[ 13 \\right], \\left[ 14 \\right] \\end{bmatrix} \\\\\n",
    "  \\begin{bmatrix} \\left[ 15 \\right], \\left[ 16 \\right], \\left[ 17 \\right] \\end{bmatrix} \n",
    "\\end{bmatrix}, \n",
    "\\ldots\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Note:** Describing a mathematical tensor as a generalized matrix is not [the whole story](https://medium.com/@quantumsteinke/whats-the-difference-between-a-matrix-and-a-tensor-4505fbdc576c). For the purposes of this introduction, this simplified definition shall, however, suffice.\n",
    "\n",
    "In PyTorch, everything runs on tensors: Your data is encoded in a tensor, the neural networks are expressed as tensors, sending the data through the network is a series of transformations on a tensor. All of these tensors are represented by a class named [`Tensor`](https://pytorch.org/docs/stable/tensors.html#torch-tensor) found in the core `torch` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2.],\n",
       "        [3., 4., 5.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.Tensor([[0, 1, 2], [3, 4, 5]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Creating a `Tensor`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are [many ways to conveniently create tensors](https://pytorch.org/docs/stable/torch.html#creation-ops) from existing data, with specific initializations, or of specific shapes. Let's try out some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A rank 3 tensor filled with zeros: \n",
      " tensor([[[0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 0.],\n",
      "         [0., 0.]]])\n",
      "A tensor of the same shape but filled with ones: \n",
      " tensor([[[1., 1.],\n",
      "         [1., 1.]],\n",
      "\n",
      "        [[1., 1.],\n",
      "         [1., 1.]]])\n",
      "A rank 2 tensor representing an identity matrix:\n",
      " tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "print(\"A rank 3 tensor filled with zeros: \\n\", zeros := torch.zeros(2, 2, 2))\n",
    "print(\"A tensor of the same shape but filled with ones: \\n\", torch.ones_like(zeros))\n",
    "print(\"A rank 2 tensor representing an identity matrix:\\n\", torch.eye(4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Indexing, slicing, operations\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the central data structure in `PyTorch`, `Tensor` objects support all features of a normal multi-dimensional array. You can access individual elements in a `Tensor` by using Python's or `numpy`'s regular indexing and slicing notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11, 12, 13],\n",
      "        [21, 22, 23],\n",
      "        [31, 32, 33]])\n",
      "Python-style indexing: tensor(23)\n",
      "Numpy-style indexing: tensor(12)\n",
      "Python-style slicing (first row): tensor([11, 12, 13])\n",
      "Numpy-style slicing (last column): tensor([13, 23, 33])\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor([[11, 12, 13], [21, 22, 23], [31, 32, 33]])\n",
    "print(t)\n",
    "print(\"Python-style indexing:\", t[1][2])\n",
    "print(\"Numpy-style indexing:\", t[0, 1])\n",
    "print(\"Python-style slicing (first row):\", t[0][:])\n",
    "print(\"Numpy-style slicing (last column):\", t[:, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform basic calculations with tensors just as we would expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = tensor([1, 1, 1]), b = tensor([2, 2, 2])\n",
      "Addition: a + b = tensor([3, 3, 3])\n",
      "Element-wise product: a * b = tensor([2, 2, 2])\n",
      "Element-wise division: a / b = tensor([0.5000, 0.5000, 0.5000])\n",
      "Matrix multiplicaiton: a @ b = tensor(6)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1, 1, 1])\n",
    "b = torch.tensor([2, 2, 2])\n",
    "\n",
    "print(f'{a = }, {b = }')\n",
    "print(f'Addition: {a + b = }')\n",
    "print(f'Element-wise product: {a * b = }')\n",
    "print(f'Element-wise division: {a / b = }')\n",
    "print(f\"Matrix multiplicaiton: {a @ b = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to basic operations, a number of methods and functions are provided to mirror the functionality offered by `numpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11, 12, 13],\n",
      "        [21, 22, 23],\n",
      "        [31, 32, 33]])\n",
      "Find the maximum value in the tensor: tensor(33)\n",
      "Find the minimum value in the tensor: tensor(11)\n",
      "Calculate the sum along the first dimension: tensor([63, 66, 69])\n",
      "Calculate the sine of each element:  tensor([[-1.0000, -0.5366,  0.4202],\n",
      "        [ 0.8367, -0.0089, -0.8462],\n",
      "        [-0.4040,  0.5514,  0.9999]])\n"
     ]
    }
   ],
   "source": [
    "print(t)\n",
    "print(\"Find the maximum value in the tensor:\", t.max())\n",
    "print(\"Find the minimum value in the tensor:\", t.min())\n",
    "print(\"Calculate the sum along the first dimension:\", t.sum(dim=0))\n",
    "print(\"Calculate the sine of each element: \", t.sin())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What makes `Tensor`s special, however, is the added functionality specifically designed for machine learning workflows.\n",
    "\n",
    "We can, for example, move a tensor to the computer's GPU. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "<b>Note:</b> If you tried to do anything GPU-related on a computer without GPU-access, your program would fail ungracefully. But you should always keep portability in mind: Machine-learning code is often developed and tested on a local machine (e.g. a laptop) and then moved to a cluster or other high-performance computer to do the actual number crunching. \n",
    "</div>\n",
    "\n",
    "There is, however, a way to make sure we only use a device that is actually available on the current machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU backend!\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():  # CUDA is usually the most desirable backend\n",
    "    backend = 'cuda'\n",
    "else:\n",
    "    backend = 'cpu'\n",
    "\n",
    "device = torch.device(backend)\n",
    "print(f'Using {backend.upper()} backend!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can safely move a tensor to a different device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = t.to(device)\n",
    "t.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all subsequent operations involving the `Tensor` are going to run on the GPU (if available)!\n",
    "\n",
    "Another major benefit of using `Tensor`s over regular `numpy` arrays is the automatic calculation of gradients, which we will learn more about in the next section. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 The `Autograd` differentiation engine\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "\n",
    "This section is in large parts taken from the [`PyTorch` tutorial \"The Fundamentals of Autograd\"](https://pytorch.org/tutorials/beginner/introyt/autogradyt_tutorial.html#what-do-we-need-autograd-for)!\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Gradients in neural network training\n",
    "Calculating gradients is *the* most important computation when training neural networks. If you would like a quick reminder why, read on. Otherwise, you can skip ahead to the [TL;DR](https://en.wikipedia.org/wiki/TL;DR) at the end of this section, or even straight to next section.\n",
    "\n",
    "A machine learning model is a function, with inputs and outputs. For this discussion, we’ll treat the inputs as an i-dimensional vector $\\vec{x}$, with elements $x_{i}$. We can then express the model, $M$, as a vector-valued function of the input: \n",
    "$$\n",
    "\\vec{y} = \\vec{M}\\left(\\vec{x}\\right) \n",
    "$$\n",
    "(We treat the value of $M$’s output as a vector because in general, a model may have any number of outputs.)\n",
    "\n",
    "Since we’ll mostly be discussing autograd in the context of training, our output of interest will be the model’s loss. The loss function \n",
    "$$\n",
    "L\\left(\\vec{y}\\right) = L\\left(\\vec{M}\\right)\n",
    "$$ \n",
    "\n",
    "is a single-valued scalar function of the model’s output. This function expresses how far off our model’s prediction was from a particular input’s ideal output. *Note:* After this point, we will often omit the vector sign where it should be contextually clear - e.g., $y$ instead of $\\vec{y}$.\n",
    "\n",
    "In training a model, we want to minimize the loss. In the idealized case of a perfect model, that means adjusting its learning weights - that is, the adjustable parameters of the function - such that loss is zero for all inputs. In the real world, it means an iterative process of nudging the learning weights until we see that we get a tolerable loss for a wide variety of inputs.\n",
    "\n",
    "How do we decide how far and in which direction to nudge the weights? We want to minimize the loss, which means making its first derivative with respect to the input equal to 0: \n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x} = 0 \n",
    "$$\n",
    "\n",
    "Recall, though, that the loss is not directly derived from the input, but a function of the model’s output (which is a function of the input directly):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x}  = \\frac{\\partial {L({\\vec y})}}{\\partial x} \n",
    "$$\n",
    "\n",
    "By the chain rule of differential calculus, we have \n",
    "\n",
    "$$\n",
    "\\frac{\\partial {L({\\vec y})}}{\\partial x} = \\frac{\\partial L}{\\partial y}\\frac{\\partial y}{\\partial x}  = \\frac{\\partial L}{\\partial y}\\frac{\\partial M(x)}{\\partial x}.$$\n",
    "\n",
    "$\\frac{\\partial M(x)}{\\partial x}$ is where things get complex. The partial derivatives of the model’s outputs with respect to its inputs, if we were to expand the expression using the chain rule again, would involve many local partial derivatives over every multiplied learning weight, every activation function, and every other mathematical transformation in the model. The full expression for each such partial derivative is the sum of the products of the local gradient of every possible path through the computation graph that ends with the variable whose gradient we are trying to measure.\n",
    "\n",
    "In particular, the gradients over the learning weights are of interest to us - they tell us what direction to change each weight to get the loss function closer to zero.\n",
    "\n",
    "Since the number of such local derivatives (each corresponding to a separate path through the model’s computation graph) will tend to go up exponentially with the depth of a neural network, so does the complexity in computing them. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "\n",
    "**TL;DR:**\n",
    "Training a neural network means adjusting its parameters so that its output is \"as correct as possible\". The gradient of the error tells us how a change of the parameters affects the correctness of the output and is therefore *the* essential driver of the training process. Calculating gradients with respect to every single parameter, however, is complex and resource-intensive.\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 `Autograd` to the rescue"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where `Autograd` comes in: It tracks the history of every computation performed on a tensor. Every computed tensor in your `PyTorch` model carries a history of its input tensors and the function used to create it. Combined with the fact that `PyTorch` functions meant to act on tensors each have a built-in implementation for computing their own derivatives, this greatly speeds up the computation of the local derivatives needed for learning.\n",
    "\n",
    "Let's look at a simple example: We will create a set of equidistant values between $0$ and $2\\pi$ and then apply a few functions to it. Afterwards we will walk *backwards* through the sequence of calculations and differentiate every step long the way.\n",
    "\n",
    "First up, we create a tensor of 25 linearly spaced values on the interval $[0, 2\\pi]$. By default, `Autograd` will not track the gradient of tensors created in this way. We have to set `requires_grad` explicitly to `True`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.2618, 0.5236, 0.7854, 1.0472, 1.3090, 1.5708, 1.8326, 2.0944,\n",
       "        2.3562, 2.6180, 2.8798, 3.1416, 3.4034, 3.6652, 3.9270, 4.1888, 4.4506,\n",
       "        4.7124, 4.9742, 5.2360, 5.4978, 5.7596, 6.0214, 6.2832],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "a = torch.linspace(0., 2. * math.pi, steps=25, requires_grad=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now do a calculation on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000e+00,  2.5882e-01,  5.0000e-01,  7.0711e-01,  8.6603e-01,\n",
       "         9.6593e-01,  1.0000e+00,  9.6593e-01,  8.6603e-01,  7.0711e-01,\n",
       "         5.0000e-01,  2.5882e-01, -8.7423e-08, -2.5882e-01, -5.0000e-01,\n",
       "        -7.0711e-01, -8.6603e-01, -9.6593e-01, -1.0000e+00, -9.6593e-01,\n",
       "        -8.6603e-01, -7.0711e-01, -5.0000e-01, -2.5882e-01,  1.7485e-07],\n",
       "       grad_fn=<SinBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.sin(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the result tensor `b` has a property called `grad_fn` that tells us that it is the result of a `sin` operation!\n",
    "\n",
    "Let's do some more computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0000e+00,  5.1764e-01,  1.0000e+00,  1.4142e+00,  1.7321e+00,\n",
      "         1.9319e+00,  2.0000e+00,  1.9319e+00,  1.7321e+00,  1.4142e+00,\n",
      "         1.0000e+00,  5.1764e-01, -1.7485e-07, -5.1764e-01, -1.0000e+00,\n",
      "        -1.4142e+00, -1.7321e+00, -1.9319e+00, -2.0000e+00, -1.9319e+00,\n",
      "        -1.7321e+00, -1.4142e+00, -1.0000e+00, -5.1764e-01,  3.4969e-07],\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([ 1.0000e+00,  1.5176e+00,  2.0000e+00,  2.4142e+00,  2.7321e+00,\n",
      "         2.9319e+00,  3.0000e+00,  2.9319e+00,  2.7321e+00,  2.4142e+00,\n",
      "         2.0000e+00,  1.5176e+00,  1.0000e+00,  4.8236e-01, -3.5763e-07,\n",
      "        -4.1421e-01, -7.3205e-01, -9.3185e-01, -1.0000e+00, -9.3185e-01,\n",
      "        -7.3205e-01, -4.1421e-01,  4.7684e-07,  4.8236e-01,  1.0000e+00],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "c = 2 * b\n",
    "print(c)\n",
    "d = c + 1\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let’s compute a single-element output, as is the case when computing a loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(25., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = d.sum()\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each `grad_fn` stored with our tensors allows you to walk the computation all the way back to its inputs with its `next_functions` property. We can drill down on this property to show us the gradient functions for all the prior tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out:\n",
      "<SumBackward0 object at 0x140c8bbb0>\n",
      "((<AddBackward0 object at 0x140c8bb80>, 0),)\n",
      "((<MulBackward0 object at 0x140c8bbb0>, 0), (None, 0))\n",
      "((<SinBackward0 object at 0x140c8bb20>, 0), (None, 0))\n",
      "((<AccumulateGrad object at 0x140c8bbb0>, 0),)\n",
      "()\n",
      "\n",
      "d:\n",
      "<AddBackward0 object at 0x140c8bb20>\n",
      "((<MulBackward0 object at 0x140c8ba90>, 0), (None, 0))\n",
      "((<SinBackward0 object at 0x140c8bb20>, 0), (None, 0))\n",
      "((<AccumulateGrad object at 0x140c8bac0>, 0),)\n",
      "()\n",
      "\n",
      "c:\n",
      "<MulBackward0 object at 0x140c8bbb0>\n",
      "\n",
      "b:\n",
      "<SinBackward0 object at 0x140c8bbb0>\n",
      "\n",
      "a:\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('out:')\n",
    "print(out.grad_fn)\n",
    "print(out.grad_fn.next_functions)\n",
    "print(out.grad_fn.next_functions[0][0].next_functions)\n",
    "print(out.grad_fn.next_functions[0][0].next_functions[0][0].next_functions)\n",
    "print(out.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions)\n",
    "print(out.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions)\n",
    "print('\\nd:')\n",
    "print(d.grad_fn)\n",
    "print(d.grad_fn.next_functions)\n",
    "print(d.grad_fn.next_functions[0][0].next_functions)\n",
    "print(d.grad_fn.next_functions[0][0].next_functions[0][0].next_functions)\n",
    "print(d.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions)\n",
    "print('\\nc:')\n",
    "print(c.grad_fn)\n",
    "print('\\nb:')\n",
    "print(b.grad_fn)\n",
    "print('\\na:')\n",
    "print(a.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `a.grad_fn` is reported as `None`, indicating that this was an input to the function with no history of its own.\n",
    "\n",
    "With all this machinery in place, how do we get derivatives out? You call the `backward()` method on the output, and check the input’s `grad` property to inspect the gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.0000e+00,  1.9319e+00,  1.7321e+00,  1.4142e+00,  1.0000e+00,\n",
      "         5.1764e-01, -8.7423e-08, -5.1764e-01, -1.0000e+00, -1.4142e+00,\n",
      "        -1.7321e+00, -1.9319e+00, -2.0000e+00, -1.9319e+00, -1.7321e+00,\n",
      "        -1.4142e+00, -1.0000e+00, -5.1764e-01,  2.3850e-08,  5.1764e-01,\n",
      "         1.0000e+00,  1.4142e+00,  1.7321e+00,  1.9319e+00,  2.0000e+00])\n"
     ]
    }
   ],
   "source": [
    "out.backward()\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will visualize this in a second, but let's try to figure out what we *should* see here. Recall that the computations we did were the following:\n",
    "\n",
    "$$\n",
    "d = 2 \\cdot \\sin\\left(a\\right) + 1\n",
    "$$\n",
    "\n",
    "So the derivative with respect to $a$ should be:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial d}{\\partial a} = 2\\cdot \\cos\\left(a\\right)\n",
    "$$\n",
    "\n",
    "Let's check this by visualizing our result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x140cc2bc0>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"402.802812pt\" height=\"297.190125pt\" viewBox=\"0 0 402.802812 297.190125\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2023-02-09T12:41:12.295956</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.6.0, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 297.190125 \nL 402.802812 297.190125 \nL 402.802812 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 38.482813 273.312 \nL 395.602813 273.312 \nL 395.602813 7.2 \nL 38.482813 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path id=\"m3066a1cb21\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m3066a1cb21\" x=\"54.71554\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(51.53429 287.910437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use xlink:href=\"#m3066a1cb21\" x=\"106.385914\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 1 -->\n      <g transform=\"translate(103.204664 287.910437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use xlink:href=\"#m3066a1cb21\" x=\"158.056288\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 2 -->\n      <g transform=\"translate(154.875038 287.910437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#m3066a1cb21\" x=\"209.726663\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 3 -->\n      <g transform=\"translate(206.545413 287.910437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use xlink:href=\"#m3066a1cb21\" x=\"261.397037\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 4 -->\n      <g transform=\"translate(258.215787 287.910437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#m3066a1cb21\" x=\"313.067411\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 5 -->\n      <g transform=\"translate(309.886161 287.910437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use xlink:href=\"#m3066a1cb21\" x=\"364.737785\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 6 -->\n      <g transform=\"translate(361.556535 287.910437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_8\">\n      <defs>\n       <path id=\"mb7b5bd3456\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#mb7b5bd3456\" x=\"38.482813\" y=\"261.216\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- −2.0 -->\n      <g transform=\"translate(7.2 265.015219) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \nL 4684 2272 \nL 4684 1741 \nL 678 1741 \nL 678 2272 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"83.789062\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"179.199219\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <g>\n       <use xlink:href=\"#mb7b5bd3456\" x=\"38.482813\" y=\"230.976\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- −1.5 -->\n      <g transform=\"translate(7.2 234.775219) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"179.199219\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#mb7b5bd3456\" x=\"38.482813\" y=\"200.736\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- −1.0 -->\n      <g transform=\"translate(7.2 204.535219) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"179.199219\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_11\">\n      <g>\n       <use xlink:href=\"#mb7b5bd3456\" x=\"38.482813\" y=\"170.496\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- −0.5 -->\n      <g transform=\"translate(7.2 174.295219) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"179.199219\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#mb7b5bd3456\" x=\"38.482813\" y=\"140.256\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.0 -->\n      <g transform=\"translate(15.579688 144.055219) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_13\">\n      <g>\n       <use xlink:href=\"#mb7b5bd3456\" x=\"38.482813\" y=\"110.016\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.5 -->\n      <g transform=\"translate(15.579688 113.815219) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_14\">\n      <g>\n       <use xlink:href=\"#mb7b5bd3456\" x=\"38.482813\" y=\"79.776\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 1.0 -->\n      <g transform=\"translate(15.579688 83.575219) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_15\">\n      <g>\n       <use xlink:href=\"#mb7b5bd3456\" x=\"38.482813\" y=\"49.536\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 1.5 -->\n      <g transform=\"translate(15.579688 53.335219) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_16\">\n      <g>\n       <use xlink:href=\"#mb7b5bd3456\" x=\"38.482813\" y=\"19.296\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 2.0 -->\n      <g transform=\"translate(15.579688 23.095219) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_17\">\n    <path d=\"M 54.71554 19.296 \nL 68.242813 23.417614 \nL 81.770085 35.501569 \nL 95.297358 54.724365 \nL 108.824631 79.776004 \nL 122.3519 108.949245 \nL 135.879176 140.256005 \nL 149.406452 171.562766 \nL 162.933722 200.736007 \nL 176.460991 225.787635 \nL 189.988261 245.010431 \nL 203.515543 257.094394 \nL 217.042813 261.216 \nL 230.570082 257.094386 \nL 244.097364 245.010424 \nL 257.624634 225.787628 \nL 271.151903 200.735989 \nL 284.679173 171.562744 \nL 298.206443 140.255999 \nL 311.733737 108.949198 \nL 325.260982 79.776007 \nL 338.788276 54.724336 \nL 352.315546 35.501555 \nL 365.842816 23.417606 \nL 379.370085 19.296 \n\" clip-path=\"url(#pfd2f262767)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 38.482813 273.312 \nL 38.482813 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 395.602813 273.312 \nL 395.602813 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 38.482813 273.312 \nL 395.602813 273.312 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 38.482813 7.2 \nL 395.602813 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pfd2f262767\">\n   <rect x=\"38.482813\" y=\"7.2\" width=\"357.12\" height=\"266.112\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We need to call the method detach() to signal that the gradients should not be tracked from this point on\n",
    "plt.plot(a.detach(), a.grad.detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎉 Success! 👏\n",
    "\n",
    "We will see more of `Autograd` in action later, but first we need to talk about how to build neural networks in `PyTorch`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Building blocks for neural networks\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure style='float:right;width:30%'>\n",
    "\n",
    "<img src=\"https://pytorch.org/assets/images/densenet1.png\">\n",
    "<figcaption>\n",
    "\n",
    "The many layers of [DenseNet](https://pytorch.org/hub/pytorch_vision_densenet/)\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Neural networks are made up of a sequence of layers. Unsurprisingly, `PyTorch` offers a great variety of layers as building blocks to string together any desired architecture. They can be found alongside various activation functions in the submodule [`torch.nn`](https://pytorch.org/docs/stable/nn.html).\n",
    "\n",
    "You can find descriptions of each layer [in the documentation](https://pytorch.org/docs/stable/nn.html#module-torch.nn), but let's quickly mention some of the basic layers together:\n",
    "\n",
    "- [`torch.nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear): A linear transformation of the incoming data:\n",
    "  $$ y = xA^\\mathrm{T} + b$$\n",
    "- [`torch.nn.Conv2d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#conv2d): A 2D convolution over the incoming data\n",
    "- [`torch.nn.LSTM`](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#lstm): Applies a long short-term memory RNN to the input data\n",
    "- [`torch.nn.Dropout`](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#dropout): A dropout layer to randomly zero some of the input values during training\n",
    "\n",
    "The number of available layer types constantly increases as new architectures are developed in the field. In addition to these, `torch.nn` contains a number of containers to facilitate composing multiple layers into a neural network, which we will do in the next section."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. A neural network from scratch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's design a neural network to classify flowers from the [Iris dataset](https://archive.ics.uci.edu/ml/datasets/iris)! \n",
    "\n",
    "We can read the data from the provided CSV file and split it into train and test set using [`pandas`](https://pandas.pydata.org/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv('data/iris.csv')\n",
    "# Encode species\n",
    "class_names = dataset['class'].unique()\n",
    "dataset['class'] = dataset['class'].map({name: idx for idx, name in enumerate(class_names)})\n",
    "\n",
    "# Split data randomly into training (90 %) and test (10 %) sets\n",
    "training_data = dataset.sample(frac=0.9)\n",
    "test_data = dataset.drop(training_data.index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the native data structure in `PyTorch` is the `Tensor` and its default data types are long integers and single-precision floats, we now convert all objects into `Tensor` of type `torch.long` or `torch.float32`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data into tensors\n",
    "training_labels = torch.tensor(training_data['class'].values, dtype=torch.long)\n",
    "training_data = torch.tensor(training_data.drop(columns='class').values, dtype=torch.float32)\n",
    "\n",
    "test_labels = torch.tensor(test_data['class'].values, dtype=torch.long)\n",
    "test_data = torch.tensor(test_data.drop(columns='class').values, dtype=torch.float32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some data, we can start designing a neural network for it. Identifying the optimal architecture for this problem is beyond the scope of this notebook. Our focus here is on understanding the building blocks of our neural network and its implementation. We could therefore try something like this:\n",
    "\n",
    "<img src=\"img/iris_network.svg\" style=\"background-color:white;padding:1em\">\n",
    "\n",
    "All neural networks (and, as a matter of fact, also all layers) are derived from the container `Module` found in `torch.nn`. We define the architecture in the subclass' `__init__` method. We also need to implement the `forward` method to define how our network is processing input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MyNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_length, hidden_layer_size, n_classes):\n",
    "        # Initialize the superclass first\n",
    "        super(MyNeuralNetwork, self).__init__()\n",
    "        \n",
    "        \"\"\" Now we can define the network's structure \"\"\"\n",
    "        # The network consists of a single, linear hidden layer...\n",
    "        self.hidden_layer = nn.Linear(input_length, hidden_layer_size)\n",
    "        # ...and an output layer\n",
    "        self.output_layer = nn.Linear(hidden_layer_size, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Here we define the networks behavior as inputs are passed through it \"\"\"\n",
    "        # The output of the hidden layer is passed through a tanh activation function\n",
    "        hidden_layer_activation = torch.tanh(self.hidden_layer(x))\n",
    "        logits = self.output_layer(hidden_layer_activation)\n",
    "        return logits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined our network architecture, we can instantiate it by specifying the hidden layer size and the number of output classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = training_data.shape[1]\n",
    "n_species = len(training_labels.unique())\n",
    "net = MyNeuralNetwork(input_length=n_features, hidden_layer_size=8, n_classes=n_species)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there it is, our first neural network in PyTorch! 🎉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyNeuralNetwork(\n",
       "  (hidden_layer): Linear(in_features=4, out_features=8, bias=True)\n",
       "  (output_layer): Linear(in_features=8, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it stands, the network is not very useful, of course. We need to train it on our dataset so that it can learn patterns in the data. Training a neural network is a high-dimensional optimization problem that seeks to minimize the deviation of the predicted classes from the actual classes. This deviation is expressed by a loss function. Many loss functions have been developed and successfully applied in the field, and PyTorch offers a wide range of loss functions to choose from in [`torch.nn`](https://pytorch.org/docs/stable/nn.html#loss-functions). The choice of the loss function is a design decision that should be carefully weighed. In this example, we will go with the cross entropy loss, which has been shown to work well with multi-class classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # Works well in this example, but there are many more options!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other important component in training is the choice of optimizer. The optimizer is essentially a strategy that determines how we update the weights and biases in our network based on a given loss. A number of optimization algorithms are implemented in the module [`torch.optim`](https://pytorch.org/docs/stable/optim.html#algorithms). Here, we will go with the popular stochastic gradient descent (SGD) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)  # Once again, there are many more options available!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we pass the parameters of our network to the optimizer here as a reference! This is how the network and the optimizer are connected: the optimizer can directly affect the networks parameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process consists of an outer loop over the epochs (complete passes through the training set) and an inner loop over all observations (or batches of observations) in the training set. Inside the inner loop, we need to do five things:\n",
    "\n",
    "1. Reset the gradients of all the parameters\n",
    "2. Let the network genereate outputs \n",
    "3. Calculate the loss between the outputs and the true labels\n",
    "4. Compute the gradients of the loss for every parameter\n",
    "5. Let the optimizer update the parameters\n",
    "\n",
    "Rinse, repeat, until we have completed all iterations.\n",
    "\n",
    "Note that before we start training, it is good practice to set the model to \"training mode\". In our example here, there is no difference between training and evaluation time. But if we had, for example, included a dropout layer, then it is very important to flag the right mode!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "net.train()\n",
    "\n",
    "print('Training', end='')\n",
    "for epoch in range(100):\n",
    "    if epoch % 10 == 0:\n",
    "        print('.', end='')\n",
    "        paired_data = list(zip(training_data, training_labels))\n",
    "        random.shuffle(paired_data)\n",
    "    for inputs, label in paired_data:\n",
    "        # Reset the optimizer so all gradients are equal to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Generate predictions\n",
    "        outputs = net(inputs)\n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, label)\n",
    "        # Computes the gradients\n",
    "        loss.backward()\n",
    "        # Optimize weights\n",
    "        optimizer.step()\n",
    "\n",
    "print('finished.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our network is trained! Now we can do a forward pass through the network and see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()  # Don't forget to set the model to evaluation mode!\n",
    "predicted_logits = net.forward(test_data)\n",
    "predicted_logits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the network does not give us a straight class label prediction. Instead, we get the output that we defined above in the forward method: The log-odds that the observation belongs to each of the three classes.\n",
    "\n",
    "We can obtain the class label by finding the most likely class, which corresponds to the index of the maximum log-odds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predicted_class = predicted_logits.max(dim=1)\n",
    "predicted_class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compare these predicted outputs to the actual class labels. Here, we will use some convenient reporting functions from `scikit-learn` (see workshop [*Intro to Machine Learning with scikit-learn*](https://git.dartmouth.edu/lib-digital-strategies/RDS/workshops/machine-learning/intro-to-machine-learning-with-scikit-learn)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "print(classification_report(test_labels, predicted_class, target_names=class_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👏 Nicely done! 💐🎉"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using a pre-trained \"off-the-shelf\" neural network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For our very simple *Iris* dataset, we could use a very simple network architecture. More complex problems, however, require more complex networks. More complex networks have more parameters, which requires more training samples and more training time. This complexity can quickly become overwhelming. \n",
    "\n",
    "Especially in computer vision and natural language processing, however, very powerful pre-trained models exist and are freely available to the general public. PyTorch makes it very easy to use such an \"off-the-shelf\" model! Not only can we quickly load the pretrained model, we can even get an entire preprocessing pipeline that makes sure our raw input is transformed into the format that the pre-trained network expects!\n",
    "\n",
    "For example, we can find a bunch of computer vision models in the module [`torchvision.models`](https://pytorch.org/vision/0.8/models.html). Another option is the [`torch.hub`](https://pytorch.org/docs/stable/hub.html#torch-hub), where you can even publish your own models!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's demonstrate using the more complex [*Flowers* dataset ](https://doi.org/10.7910/DVN/1ECTVN). This dataset consists of labeled `*.jpg` images of five different kinds of flowers (see the [dataset's repository](https://git.dartmouth.edu/lib-digital-strategies/RDS/datasets/flowers-dataset)). Here, we will only use the daisies and roses to keep training times down, but feel free to experiment with the whole dataset!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "\n",
    "If you are running this notebook as part of a Reproducible Research workshop JupyterHub session, you can find the dataset in `/shared/RR-workshop-data/flowers-dataset`. If you are running this outside of Dartmouth's JupyterHub, you can download the [dataset here](https://git.dartmouth.edu/lib-digital-strategies/RDS/datasets/flowers-dataset/-/archive/main/flowers-dataset-main.zip). Unzip it to a location of your choosing and adjust the dataset path in the next cell accordingly!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# If you are on Dartmouth's JupyterHub, the data is here:\n",
    "dataset_root = Path('~/shared/RR-workshop-data/flowers-dataset').expanduser()\n",
    "    \n",
    "# If you are running this notebook anywhere else, set the path here:\n",
    "# dataset_root = Path('path/to/dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load a model architecture initialize it with the pretrained weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
    "\n",
    "weights = MobileNet_V2_Weights.DEFAULT  # We even have different sets of pre-trained weights available! We'll go with the default here\n",
    "net = mobilenet_v2(weights=weights)\n",
    "net"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that's a complex network! 😱 🤯"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we grab the preprocessing pipeline from the pre-trained weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = weights.transforms()\n",
    "preprocess"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the preprocessing consists of cropping and then resizing. We also get the mean and standard deviation from the training set used to train the weights that are used to standardize.\n",
    "\n",
    "\n",
    "Let's get some boilerplate code to read random image file and display it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "\n",
    "\n",
    "def get_random_image(test=False, classes=['roses', 'daisy']):\n",
    "    dataset = dataset_root\n",
    "    images = []\n",
    "    for class_ in classes:\n",
    "        subfolder = dataset / f\"{'test' if test else 'train'}\" / class_\n",
    "        images += [img for img in subfolder.glob('*.jpg')]\n",
    "    return read_image(str(random.choice(images)), ImageReadMode.RGB)\n",
    "\n",
    "def imshow(inp, title=None):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some random images by running the next cell multiple times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = get_random_image()\n",
    "imshow(img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that they vary in size and aspect ratio. Now let's look what happens when we preprocess them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = preprocess(img)\n",
    "imshow(img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha, they are now resized and cropped to the same size and standardized (which means the colors look weird for the human eye)!\n",
    "\n",
    "Time to let the network do its thing and predict the label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "\n",
    "img = get_random_image()\n",
    "imshow(img)\n",
    "img = preprocess(img)\n",
    "\n",
    "prediction = net(img.unsqueeze(0)).squeeze(0)\n",
    "class_id = prediction.argmax().item()\n",
    "score = prediction[class_id].item()\n",
    "category_name = weights.meta[\"categories\"][class_id]\n",
    "\n",
    "print(category_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🤨 Hmmm, that does not look right. 🤔\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Why do you think this highly sophisticated network does not give us the right answers?\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Adapting an off-the-shelf neural network using transfer learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your problem happens to align with the problem that a network was trained to solve (e.g., your labels of interest are a subset of the labels used to train the model), you are in luck and can usually use a pre-trained network out-of-the-box. \n",
    "\n",
    "Often, however, your problem is slightly different. In this example, we are using photographic images just like `MobileNet`, but their labels are different than what `MobileNet` was originally trained with.\n",
    "\n",
    "Pre-trained networks can still be very valuable, however. You could think of a pre-trained network as a very sophisticated feature extractor:\n",
    "\n",
    "If we think of features as simply *numbers that describe an object*, then we could consider the output of any layer of the network, which are *numbers* that change depending on the network's input (the *object*), as features! From that point of view, the layers of a network are essentially a series of transforms of the input image to get to a feature representation. This feature vector is then classified by the final layer.\n",
    "\n",
    "So we can swap out that final classifier layer, use the very same features that `MobileNet` was trained to extract, and simply classify them with a classifier fitting our own problem (i.e., using the class labels we are interested in).\n",
    "\n",
    "We use most of the layers of `MobileNet` but insert our own final classification layer (or an entire other network!). We can then use the original layers as-is and only train the final layer. Since this means that we have way fewer parameters to train, we also need way fewer training samples.\n",
    "\n",
    "Let's walk through the process using our *Flowers* dataset, which is conveniently already split into a train and test subset.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Remember:** We will only use the images of daisies and roses in this tutorial to keep the training time down. You are welcome to try to classify all five flowers contained in the dataset, though!\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will create a list of all the image files we want to use for training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_image_paths = []\n",
    "test_image_paths = []\n",
    "\n",
    "for class_ in ['daisy', 'roses']:\n",
    "    folder = dataset_root / 'train' / class_\n",
    "    training_image_paths += [file for file in folder.glob('*.jpg')]\n",
    "    folder = dataset_root / 'test' / class_\n",
    "    test_image_paths += [file for file in folder.glob('*.jpg')]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will read and preprocess all the images and store them in a `Tensor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor holding all the training images...\n",
    "training_data = torch.stack([preprocess(read_image(str(file))) for file in training_image_paths])\n",
    "class_labels = ['daisy', 'roses']\n",
    "training_labels = torch.stack([torch.tensor(class_labels.index(str(file.parent.name))) for file in training_image_paths])\n",
    "\n",
    "# ...and one for the test images\n",
    "test_data = torch.stack([preprocess(read_image(str(file))) for file in test_image_paths])\n",
    "test_labels = torch.stack([torch.tensor(class_labels.index(str(file.parent.name))) for file in test_image_paths])\n",
    "\n",
    "print(f\"We have {len(training_data)} training images and {len(test_data)} test images.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Pro question:** What shape should the tensor `training_data` have now?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will want to make sure that all tensors are moved to the chosen device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data.to(device)\n",
    "training_labels = training_labels.to(device)\n",
    "test_data = test_data.to(device)\n",
    "test_labels = test_labels.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to swap `MobileNet`'s classifier for an untrained one that fits our problem.\n",
    "\n",
    "First we make sure that none of `MobileNet`'s parameters get trained (since they are pre-trained):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in net.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we swap in our simple, untrained classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The new classifier will take the same number of inputs as the old classifier\n",
    "n_features = net.classifier[1].in_features\n",
    "# Replace the original classifier with one that has the appropriate number of outputs \n",
    "net.classifier[1] = nn.Linear(n_features, len(class_labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we move the network to the chosen device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training is now basically the same as before. We need to define a loss and an optimizer and pass the parameters we want to optimize to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.classifier.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are only passing the classifier's parameters to the optimizer! Everything else would not get optimized, anyway (see above).\n",
    "\n",
    "Now we can actually do the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "since = time.time()\n",
    "\n",
    "num_epochs = 1 \n",
    "\n",
    "# Don't forget to set the network to training mode!\n",
    "net.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    # Iterate over the data\n",
    "    paired_data = list(zip(training_data, training_labels))\n",
    "    random.shuffle(paired_data)\n",
    "    for idx, (input, label) in enumerate(paired_data):\n",
    "        if idx % 100 == 0:\n",
    "            print('.', end='')\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calculate predictions\n",
    "        outputs = net(input.unsqueeze(0)).squeeze(0)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, label)\n",
    "        \n",
    "        # Calculate the gradients        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimize the parameters\n",
    "        optimizer.step()\n",
    "    print(\"\")\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Note:** We only did a single epoch here to keep the training time down. Feel free to do some additional epochs and see how it affects the networks performance!\n",
    "</div>\n",
    "\n",
    "Now that the network is trained, we can score its accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "\n",
    "predictions = net(test_data)\n",
    "print(classification_report(y_pred=predictions.argmax(1), \n",
    "                            y_true=test_labels, target_names=class_labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad for this relatively small amount of data! 🌹 🌼\n",
    "\n",
    "Let's look at some random examples again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = get_random_image(test=True)\n",
    "imshow(img)\n",
    "img = preprocess(img)\n",
    "\n",
    "prediction = net(img.unsqueeze(0)).squeeze(0).softmax(0)\n",
    "class_id = prediction.argmax().item()\n",
    "score = prediction[class_id].item()\n",
    "category_name = class_labels[class_id]\n",
    "\n",
    "print(f\"{category_name}: {100 * score:.1f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Far from perfect, but definitely a great start! With some additional tuning, we would have a pretty solid, complex classifier from very little data!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Next steps\n",
    "\n",
    "- [Official `PyTorch` tutorials](https://pytorch.org/tutorials/index.html)\n",
    "- The concept of [Datasets & Dataloaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
    "- Batch processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table >\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td style=\"padding:0px;border-width:0px;vertical-align:center\">    \n",
    "    Created by Simon Stone for Dartmouth College Library under <a href=\"https://creativecommons.org/licenses/by/4.0/\">Creative Commons CC BY-NC 4.0 License</a>.<br>For questions, comments, or improvements, email <a href=\"mailto:researchdatahelp@groups.dartmouth.edu\">Research Data Services</a>.\n",
    "    </td>\n",
    "    <td style=\"padding:0 0 0 1em;border-width:0px;vertical-align:center\"><img alt=\"Creative Commons License\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\"/></td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
