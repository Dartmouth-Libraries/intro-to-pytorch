{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to PyTorch\n",
    "<figure style='float:right;max-width:30%;'>\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/c/c6/PyTorch_logo_black.svg/640px-PyTorch_logo_black.svg.png' style='padding:10px;background-color:white'>\n",
    "<figcaption style='text-align:right'>Source: <a href=https://commons.wikimedia.org/wiki/File:PyTorch_logo_black.svg>Wikimedia Commons</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "PyTorch is a machine learning framework with a major focus on neural networks used for computer vision, audio and natural language processing. The user-facing frontend is written in Python, but the number-crunching is handled by a more optimized C++ backend, including support for outsourcing computations to graphics cards (GPUs) for a substantial increase in speed. PyTorch was originally created by Meta (formerly known as facebook), but has always been open source, permissively licensed ([BSD-3](https://en.wikipedia.org/wiki/BSD_licenses#3-clause)), and since September 2022 is managed by the non-profit PyTorch Foundation, a subsidiary of the [Linux Foundation](https://en.wikipedia.org/wiki/Linux_Foundation).\n",
    "\n",
    "The accessible interface, huge community, and optimized implementations have established PyTorch among the top choices for education, research, and production in the field of neural network design.\n",
    "\n",
    "**Caveat emptor:** PyTorch is not really \"better\" or \"worse\" than other popular frameworks like [Keras](https://keras.io/) or [TensorFlow](https://www.tensorflow.org/). While each framework has its particular strengths, they differ more in their style, philosophy, and user base than in their feature lists and performance. You should absolutely explore other options available to you and find what you like best!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure\n",
    "\n",
    "The core package of PyTorch is called [`torch`](https://pypi.org/project/torch/). This package contains all the code required to setup and compute general purpose neural networks. It is extended by packages that offer more specialized functions and objects specific to various applications: [`torchvision`](https://pytorch.org/vision/stable/index.html) for Computer Vision (working with images or videos), [`torchaudio`](https://pytorch.org/audio/stable/index.html) for audio processing (e.g. speech recognition or synthesis), and [`torchtext`](https://pytorch.org/text/stable/index.html) for natural language processing. PyTorch is extended by various other packages that comprise the [*PyTorch Ecosystem*](https://pytorch.org/ecosystem/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core components\n",
    "\n",
    "A plethora of functions and objects can be found within PyTorch. But arguably the most important basic components are:\n",
    "\n",
    "1. The [`Tensor`](https://pytorch.org/docs/stable/tensors.html#tensor-class-reference) class\n",
    "2. The differentiation engine [`Autograd`](https://pytorch.org/docs/stable/autograd.html#module-torch.autograd)\n",
    "3. The neural network building blocks (layers and activation functions) found in [`torch.nn`](https://pytorch.org/docs/stable/nn.html#module-torch.nn)\n",
    "\n",
    "Before we build our first neural network from scratch, let us walk through these components one at a time:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `Tensor` class\n",
    "<figure style='float:right;max-width=10%;'>\n",
    "<img src=https://imgs.xkcd.com/comics/machine_learning.png style='padding-right:10px'>\n",
    "<figcaption>Source: <a href=https://xkcd.com/license.html>XKCD</a> </figcaption>\n",
    "</figure>\n",
    "\n",
    "Neural networks are essentially a sequence of linear algebra operations. A [mathematical tensor](https://en.wikipedia.org/wiki/Tensor) is the most general algebraic object, of which simpler algebraic objects can be derived:\n",
    "\n",
    "- A scalar is a tensor of rank 0:\n",
    "$$\n",
    "\\left[ 0 \\right]\n",
    "$$\n",
    "- A vector is a tensor of rank 1 (a.k.a. a collection of rank 0 tensors):\n",
    "$$\n",
    "\\begin{bmatrix} \\left[ 0 \\right], \\left[ 1 \\right], \\left[ 2 \\right] \\end{bmatrix}\n",
    "$$\n",
    "- A matrix is a tensor of rank 2 (a.k.a. a collection of rank 1 tensors):\n",
    "$$\n",
    "\\begin{bmatrix} \n",
    "  \\begin{bmatrix} \\left[ 0 \\right], \\left[ 1 \\right], \\left[ 2 \\right] \\end{bmatrix} \\\\\n",
    "  \\begin{bmatrix} \\left[ 3 \\right], \\left[ 4 \\right], \\left[ 5 \\right] \\end{bmatrix} \\\\\n",
    "  \\begin{bmatrix} \\left[ 6 \\right], \\left[ 7 \\right], \\left[ 8 \\right] \\end{bmatrix} \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "- An $n$-dimensional array is a tensor of rank $n$ (a.k.a. a collection of rank $n-1$ tensors):\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "  \\begin{bmatrix} \\left[ 0 \\right], \\left[ 1 \\right], \\left[ 2 \\right] \\end{bmatrix} \\\\\n",
    "  \\begin{bmatrix} \\left[ 3 \\right], \\left[ 4 \\right], \\left[ 5 \\right] \\end{bmatrix} \\\\\n",
    "  \\begin{bmatrix} \\left[ 6 \\right], \\left[ 7 \\right], \\left[ 8 \\right] \\end{bmatrix} \n",
    "\\end{bmatrix}, \n",
    "\\begin{bmatrix} \n",
    "  \\begin{bmatrix} \\left[ 9 \\right], \\left[ 10 \\right], \\left[ 11 \\right] \\end{bmatrix} \\\\\n",
    "  \\begin{bmatrix} \\left[ 12 \\right], \\left[ 13 \\right], \\left[ 14 \\right] \\end{bmatrix} \\\\\n",
    "  \\begin{bmatrix} \\left[ 15 \\right], \\left[ 16 \\right], \\left[ 17 \\right] \\end{bmatrix} \n",
    "\\end{bmatrix}, \n",
    "\\ldots\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Note:** Describing a mathematical tensor as a generalized matrix is not [the whole story](https://medium.com/@quantumsteinke/whats-the-difference-between-a-matrix-and-a-tensor-4505fbdc576c). For the purposes of this introduction, this simplified definition shall, however, suffice.\n",
    "\n",
    "In PyTorch, everything runs on tensors: Your data is encoded in a tensor, the neural networks are expressed as tensors, sending the data through the network is a series of transformations on a tensor. All of these tensors are represented by a class named [`Tensor`](https://pytorch.org/docs/stable/tensors.html#torch-tensor) found in the core `torch` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "torch.Tensor([[0, 1, 2], [3, 4, 5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a `Tensor`\n",
    "There are [many ways to conveniently create tensors](https://pytorch.org/docs/stable/torch.html#creation-ops) from existing data, with specific initializations, or of specific shapes. Let's try out some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"A rank 3 tensor filled with zeros: \\n\", zeros := torch.zeros(2, 2, 2))\n",
    "print(\"A tensor of the same shape but filled with ones: \\n\", torch.ones_like(zeros))\n",
    "print(\"A rank 2 tensor representing an identity matrix:\\n\", torch.eye(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing, slicing, operations\n",
    "\n",
    "As the central data structure in `PyTorch`, `Tensor` objects support all features of a normal multi-dimensional array. You can access individual elements in a `Tensor` by using Python's or `numpy`'s regular indexing and slicing notation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([[11, 12, 13], [21, 22, 23], [31, 32, 33]])\n",
    "print(t)\n",
    "print(\"Python-style indexing:\", t[1][2])\n",
    "print(\"Numpy-style indexing:\", t[0, 1])\n",
    "print(\"Python-style slicing (first row):\", t[0][:])\n",
    "print(\"Numpy-style slicing (last column):\", t[:, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform basic calculations with tensors just as we would expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1, 1, 1])\n",
    "b = torch.tensor([2, 2, 2])\n",
    "\n",
    "print(f'{a = }, {b = }')\n",
    "print(f'Addition: {a + b = }')\n",
    "print(f'Element-wise product: {a * b = }')\n",
    "print(f'Element-wise division: {a / b = }')\n",
    "print(f\"Matrix multiplicaiton: {a @ b = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to basic operations, a number of methods and functions are provided to mirror the functionality offered by `numpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Find the maximum value in the tensor:\", t.max())\n",
    "print(\"Find the minimum value in the tensor:\", t.min())\n",
    "print(\"Calculate the sum along the first dimension:\", t.sum(dim=0))\n",
    "print(\"Calculate the sine of each element: \", t.sin())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What makes `Tensor`s special, however, is the added functionality specifically designed for machine learning workflows.\n",
    "\n",
    "We can, for example, move a tensor to the computer's GPU. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "<b>Note:</b> If you tried to do anything GPU-related on a computer without GPU-access, your program would fail ungracefully. But you should always keep portability in mind: Machine-learning code is often developed and tested on a local machine (e.g. a laptop) and then moved to a cluster or other high-performance computer to do the actual number crunching. \n",
    "</div>\n",
    "\n",
    "There is, however, a way to make sure we only use a device that is actually available on the current machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():  # CUDA is usually the most desirable backend\n",
    "    backend = 'cuda'\n",
    "elif torch.backends.mps.is_available():  # MPS is supported by some MacOS devices\n",
    "    backend = 'mps'\n",
    "else:\n",
    "    backend = 'cpu'\n",
    "\n",
    "device = torch.device(backend)\n",
    "print(f'Using {backend.upper()} backend!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can safely move a tensor to a different device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = t.to(device)\n",
    "t.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all subsequent operations involving the `Tensor` are going to run on the GPU (if available)!\n",
    "\n",
    "Another major benefit of using `Tensor`s over regular `numpy` arrays is the automatic calculation of gradients, which we will learn more about in the next section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `Autograd` differentiation engine\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "\n",
    "This section is in large parts taken from the [`PyTorch` tutorial \"The Fundamentals of Autograd\"](https://pytorch.org/tutorials/beginner/introyt/autogradyt_tutorial.html#what-do-we-need-autograd-for)!\n",
    "</div>\n",
    "\n",
    "#### Gradients in neural network training\n",
    "Calculating gradients is *the* most important computation when training neural networks. If you would like a quick reminder why, read on. Otherwise, you can skip ahead to the next section.\n",
    "\n",
    "A machine learning model is a function, with inputs and outputs. For this discussion, we‚Äôll treat the inputs as an i-dimensional vector $\\vec{x}$, with elements $x_{i}$. We can then express the model, $M$, as a vector-valued function of the input: \n",
    "$$\n",
    "\\vec{y} = \\vec{M}\\left(\\vec{x}\\right) \n",
    "$$\n",
    "(We treat the value of $M$‚Äôs output as a vector because in general, a model may have any number of outputs.)\n",
    "\n",
    "Since we‚Äôll mostly be discussing autograd in the context of training, our output of interest will be the model‚Äôs loss. The loss function \n",
    "$$\n",
    "L\\left(\\vec{y}\\right) = L\\left(\\vec{M}\\right)\n",
    "$$ \n",
    "\n",
    "is a single-valued scalar function of the model‚Äôs output. This function expresses how far off our model‚Äôs prediction was from a particular input‚Äôs ideal output. *Note:* After this point, we will often omit the vector sign where it should be contextually clear - e.g., $y$ instead of $\\vec{y}$.\n",
    "\n",
    "In training a model, we want to minimize the loss. In the idealized case of a perfect model, that means adjusting its learning weights - that is, the adjustable parameters of the function - such that loss is zero for all inputs. In the real world, it means an iterative process of nudging the learning weights until we see that we get a tolerable loss for a wide variety of inputs.\n",
    "\n",
    "How do we decide how far and in which direction to nudge the weights? We want to minimize the loss, which means making its first derivative with respect to the input equal to 0: \n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x} = 0 \n",
    "$$\n",
    "\n",
    "Recall, though, that the loss is not directly derived from the input, but a function of the model‚Äôs output (which is a function of the input directly):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x}  = \\frac{\\partial {L({\\vec y})}}{\\partial x} \n",
    "$$\n",
    "\n",
    "By the chain rule of differential calculus, we have \n",
    "\n",
    "$$\n",
    "\\frac{\\partial {L({\\vec y})}}{\\partial x} = \\frac{\\partial L}{\\partial y}\\frac{\\partial y}{\\partial x}  = \\frac{\\partial L}{\\partial y}\\frac{\\partial M(x)}{\\partial x}.$$\n",
    "\n",
    "$\\frac{\\partial M(x)}{\\partial x}$ is where things get complex. The partial derivatives of the model‚Äôs outputs with respect to its inputs, if we were to expand the expression using the chain rule again, would involve many local partial derivatives over every multiplied learning weight, every activation function, and every other mathematical transformation in the model. The full expression for each such partial derivative is the sum of the products of the local gradient of every possible path through the computation graph that ends with the variable whose gradient we are trying to measure.\n",
    "\n",
    "In particular, the gradients over the learning weights are of interest to us - they tell us what direction to change each weight to get the loss function closer to zero.\n",
    "\n",
    "Since the number of such local derivatives (each corresponding to a separate path through the model‚Äôs computation graph) will tend to go up exponentially with the depth of a neural network, so does the complexity in computing them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Autograd` to the rescue\n",
    "\n",
    "This is where `Autograd` comes in: It tracks the history of every computation performed on a tensor. Every computed tensor in your `PyTorch` model carries a history of its input tensors and the function used to create it. Combined with the fact that `PyTorch` functions meant to act on tensors each have a built-in implementation for computing their own derivatives, this greatly speeds the computation of the local derivatives needed for learning.\n",
    "\n",
    "Let's look at a simple example: We will create a set of equidistant values between $0$ and $2\\pi$ and then apply a few functions to it. Afterwards we will walk *backwards* through the sequence of calculations and differentiate every step long the way.\n",
    "\n",
    "First up, we create a tensor of 25 linearly spaced values on the interval $[0, 2\\pi]$. By default, `Autograd` will not track the gradient of tensors created in this way. We have to set `requires_grad` explicitly to `True`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "a = torch.linspace(0., 2. * math.pi, steps=25, requires_grad=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now do a calculation on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.sin(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the result tensor `b` has a property called `grad_fn` that tells us that it is the result of a `sin` operation!\n",
    "\n",
    "Let's do some more computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 2 * b\n",
    "print(c)\n",
    "d = c + 1\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let‚Äôs compute a single-element output, as is the case when computing a loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = d.sum()\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each `grad_fn` stored with our tensors allows you to walk the computation all the way back to its inputs with its `next_functions` property. We can drill down on this property to show us the gradient functions for all the prior tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('out:')\n",
    "print(out.grad_fn)\n",
    "print(out.grad_fn.next_functions)\n",
    "print(out.grad_fn.next_functions[0][0].next_functions)\n",
    "print(out.grad_fn.next_functions[0][0].next_functions[0][0].next_functions)\n",
    "print(out.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions)\n",
    "print(out.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions)\n",
    "print('\\nd:')\n",
    "print(d.grad_fn)\n",
    "print(d.grad_fn.next_functions)\n",
    "print(d.grad_fn.next_functions[0][0].next_functions)\n",
    "print(d.grad_fn.next_functions[0][0].next_functions[0][0].next_functions)\n",
    "print(d.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions)\n",
    "print('\\nc:')\n",
    "print(c.grad_fn)\n",
    "print('\\nb:')\n",
    "print(b.grad_fn)\n",
    "print('\\na:')\n",
    "print(a.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `a.grad_fn` is reported as `None`, indicating that this was an input to the function with no history of its own.\n",
    "\n",
    "With all this machinery in place, how do we get derivatives out? You call the `backward()` method on the output, and check the input‚Äôs `grad` property to inspect the gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will visualize this in a second, but let's try to figure out what we *should* see here. Recall that the computations we did were the following:\n",
    "\n",
    "$$\n",
    "d = 2 \\cdot \\sin\\left(a\\right) + 1\n",
    "$$\n",
    "\n",
    "So the derivative with respect to $a$ should be:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial d}{\\partial a} = 2\\cdot \\cos\\left(a\\right)\n",
    "$$\n",
    "\n",
    "Let's check this by visualizing our result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We need to call the method detach() to signal that the gradients should not be tracked from this point on\n",
    "plt.plot(a.detach(), a.grad.detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéâ Success! üëè\n",
    "\n",
    "We will see more of `Autograd` in action later, but first we need to talk about how to build neural networks in `PyTorch`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building blocks for neural networks\n",
    "\n",
    "<figure style='float:right;width:30%'>\n",
    "\n",
    "<img src=\"https://pytorch.org/assets/images/densenet1.png\">\n",
    "<figcaption>\n",
    "\n",
    "The many layers of [DenseNet](https://pytorch.org/hub/pytorch_vision_densenet/)\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Neural networks are made up of a sequence of layers. Unsurprisingly, `PyTorch` offers a great variety of layers as building blocks to string together any desired architecture. They can be find alongside various activation functions in the submodule [`torch.nn`](https://pytorch.org/docs/stable/nn.html).\n",
    "\n",
    "You can find descriptions of each layer [in the documentation](https://pytorch.org/docs/stable/nn.html#module-torch.nn), but let's quickly mention some of the basic layers together:\n",
    "\n",
    "- [`torch.nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear): A linear transformation of the incoming data:\n",
    "  $$ y = xA^\\mathrm{T} + b$$\n",
    "- [`torch.nn.Conv2d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#conv2d): A 2D convolution over the incoming data\n",
    "- [`torch.nn.LSTM`](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#lstm): Applies a long short-term memory RNN to the input data\n",
    "- [`torch.nn.Dropout`](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#dropout): A dropout layer to randomly zero some of the input values during training\n",
    "\n",
    "The number of available layer types constantly increases as new architectures are developed in the field. In addition to these, `torch.nn` contains a number of containers to facilitate composing multiple layers into a neural network, which we will do in the next section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A neural network from scratch\n",
    "Let's design a neural network to classify flowers from the [Iris dataset](https://archive.ics.uci.edu/ml/datasets/iris)! \n",
    "\n",
    "We can read the data from the provided CSV file and split it into train and test set using [`pandas`](https://pandas.pydata.org/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv('data/iris.csv')\n",
    "# Encode species\n",
    "class_names = dataset['class'].unique()\n",
    "dataset['class'] = dataset['class'].map({name: idx for idx, name in enumerate(class_names)})\n",
    "\n",
    "# Split data randomly into training (90 %) and test (10 %) sets\n",
    "training_data = dataset.sample(frac=0.9)\n",
    "test_data = dataset.drop(training_data.index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some data, we can start designing a neural network for it. Identifying the optimal architecture for this problem is beyond the scope of this notebook. Our focus here is on understanding the building blocks of our neural network and its implementation. We could therefore try something like this:\n",
    "\n",
    "<img src=\"img/iris_network.svg\" style=\"background-color:white;padding:1em\">\n",
    "\n",
    "All neural networks (and, as a matter of fact, also all layers) are derived from the container `Module` found in `torch.nn`. We define the architecture in the subclass' `__init__` method. We also need to implement the `forward` method to define how our network is processing input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MyNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_length, hidden_layer_size, n_classes):\n",
    "        # Initialize the superclass first\n",
    "        super(MyNeuralNetwork, self).__init__()\n",
    "        \n",
    "        \"\"\" Now we can define the network's structure \"\"\"\n",
    "        # The network consists of a single, linear hidden layer...\n",
    "        self.hidden_layer = nn.Linear(input_length, hidden_layer_size)\n",
    "        # ...and an output layer\n",
    "        self.output_layer = nn.Linear(hidden_layer_size, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Here we define the networks behavior as inputs are passed through it \"\"\"\n",
    "        # The output of the hidden layer is passed through a tanh activation function\n",
    "        hidden_layer_activation = torch.tanh(self.hidden_layer(x))\n",
    "        logits = self.output_layer(hidden_layer_activation)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = len(dataset.columns) - 1 \n",
    "n_species = dataset['class'].nunique()\n",
    "net = MyNeuralNetwork(input_length=n_features, hidden_layer_size=8, n_classes=n_species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training', end='')\n",
    "for epoch in range(100):\n",
    "    if epoch % 10 == 0:\n",
    "        print('.', end='')\n",
    "    for _, observation in training_data.sample(frac=1).iterrows():\n",
    "        inputs = torch.tensor(observation[['sepal length in cm', 'sepal width in cm', 'petal length in cm', 'petal width in cm']].values)\n",
    "        label = torch.tensor(observation['class'].astype('long'))\n",
    "\n",
    "        # Reset the optimizer so all gradients are equal to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Generate predictions\n",
    "        outputs = net(inputs)\n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, label)\n",
    "        # Computes the gradients\n",
    "        loss.backward()\n",
    "        # Optimize weights\n",
    "        optimizer.step()\n",
    "\n",
    "print('finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_proba = net.forward(torch.tensor(test_data.iloc[:, :-1].values))\n",
    "\n",
    "_, predicted_class = predicted_proba.max(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "print(classification_report(test_data['class'], predicted_class, target_names=class_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a pre-trained \"off-the-shelf\" neural network\n",
    "\n",
    "Data used: \n",
    "\n",
    "@data{DVN/1ECTVN_2020,\n",
    "author = {Tung, K},\n",
    "publisher = {Harvard Dataverse},\n",
    "title = {{Flowers Dataset}},\n",
    "UNF = {UNF:6:z6JGwpi2tftxFU+tbVH/3g==},\n",
    "year = {2020},\n",
    "version = {V8},\n",
    "doi = {10.7910/DVN/1ECTVN},\n",
    "url = {https://doi.org/10.7910/DVN/1ECTVN}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
    "\n",
    "weights = MobileNet_V2_Weights.DEFAULT\n",
    "net = mobilenet_v2(weights=weights).float()\n",
    "preprocess = weights.transforms()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image, ImageReadMode\n",
    "import numpy as np\n",
    "\n",
    "img = read_image('data/flowers/flower_photos/train/daisy/5547758_eea9edfd54_n.jpg', ImageReadMode.RGB)\n",
    "\n",
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    \n",
    "batch = preprocess(img).unsqueeze(0)\n",
    "\n",
    "imshow(batch.squeeze(0))\n",
    "\n",
    "prediction = net(batch).squeeze(0).softmax(0)\n",
    "class_id = prediction.argmax().item()\n",
    "score = prediction[class_id].item()\n",
    "category_name = weights.meta[\"categories\"][class_id]\n",
    "print(f\"{category_name}: {100 * score:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapting an off-the-shelf neural network using transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# Create a dataset metadata file\n",
    "dataset = pd.DataFrame([file for file in Path(\"data/flowers/flower_photos/\").rglob('*.jpg')], columns=['path'])\n",
    "dataset['species'] = dataset.path.apply(lambda x: str(x.parent.name))\n",
    "dataset['subset'] = 'train'\n",
    "# Put a one sample per species aside for testing\n",
    "test_set = dataset.groupby(by='species').sample(n=1).index\n",
    "dataset.loc[test_set, 'subset'] = 'test'\n",
    "\n",
    "# Create a 0-based index of the class labels\n",
    "class_names = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']\n",
    "dataset['class_id'] = dataset['species'].map({name: idx for idx, name in enumerate(class_names)}).astype('long')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_model(model, preprocess, training_data, n_batches, criterion, optimizer, scheduler=None, num_epochs=25):\n",
    "    since = time.time()\n",
    "    losses = []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Iterate over data in  batches\n",
    "        for batch in np.array_split(training_data.sample(frac=1), n_batches):\n",
    "            print('.', end='')\n",
    "            inputs = []\n",
    "            labels = []\n",
    "            for _, meta in batch.iterrows():\n",
    "                img = read_image(str(meta['path']), ImageReadMode.RGB)\n",
    "                img = preprocess(img)\n",
    "                inputs.append(img)\n",
    "                labels.append(torch.tensor(meta['class_id']))\n",
    "            inputs = torch.stack(inputs)    \n",
    "            labels = torch.stack(labels)\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            losses.append(loss.detach())\n",
    "            print(f\"Loss: {loss}\")\n",
    "\n",
    "            # backward + optimize only if in training phase\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "        print(\"\")\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    \n",
    "    plt.plot(losses)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# Replace the classifier layer\n",
    "n_features = net.classifier[1].in_features\n",
    "net.classifier[1] = nn.Linear(n_features, dataset['species'].nunique())\n",
    "net = net.float()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.classifier.parameters(), lr=0.01)\n",
    "\n",
    "net = train_model(model=net, preprocess=preprocess, training_data=dataset.query('subset == \"train\"'), n_batches=10, criterion=criterion, optimizer=optimizer, num_epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "\n",
    "for _, meta in dataset.query('subset == \"test\"').iterrows():\n",
    "    img = read_image(str(meta['path']), ImageReadMode.RGB)\n",
    "    batch = preprocess(img).unsqueeze(0)\n",
    "\n",
    "    prediction = net(batch).squeeze(0).softmax(0)\n",
    "    class_id = prediction.argmax().item()\n",
    "    score = prediction[class_id].item()\n",
    "    print(f\"{class_names[class_id]}: {100 * score:.1f}%, True class: {meta['species']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "[Official `PyTorch` tutorials](https://pytorch.org/tutorials/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table >\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td style=\"padding:0px;border-width:0px;vertical-align:center\">    \n",
    "    Created by Simon Stone for Dartmouth College Library under <a href=\"https://creativecommons.org/licenses/by/4.0/\">Creative Commons CC BY-NC 4.0 License</a>.<br>For questions, comments, or improvements, email <a href=\"mailto:researchdatahelp@groups.dartmouth.edu\">Research Data Services</a>.\n",
    "    </td>\n",
    "    <td style=\"padding:0 0 0 1em;border-width:0px;vertical-align:center\"><img alt=\"Creative Commons License\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\"/></td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
